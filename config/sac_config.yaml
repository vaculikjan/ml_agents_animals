behaviors:
  simple_deer:
    trainer_type: sac
    hyperparameters:
      learning_rate: 0.0001  # A moderate learning rate for stable learning
      learning_rate_schedule: constant  # Gradually decrease the learning rate over time
      batch_size: 128  # A larger batch size for more stable gradient estimates
      buffer_size: 500000  # A larger buffer to store more experiences
      buffer_init_steps: 10000  # Populate the buffer with some experiences before learning starts
      tau: 0.005  # Rate at which the target networks are updated
      steps_per_update: 20.0  # More frequent updates may help in a complex environment
      save_replay_buffer: false
      init_entcoef: 0.01  # A lower initial entropy coefficient may encourage more exploration initially
      reward_signal_steps_per_update: 10.0
    network_settings:
      normalize: true  # Normalizing input features can help in diverse environments
      hidden_units: 64  # More hidden units to capture complex behaviors
      num_layers: 2  # Adding an extra layer can help in learning complex policies
      vis_encode_type: simple
    reward_signals:
      extrinsic:
        gamma: 0.95  # A slightly lower gamma to balance short-term and long-term rewards
        strength: 1.0
    keep_checkpoints: 10  # Keeping more checkpoints for better progress tracking
    max_steps: 1000000  # A higher max steps to allow more time for learning
    time_horizon: 64  # This should be sufficient for most tasks
    summary_freq: 10000  # Adjusting the frequency of summaries
